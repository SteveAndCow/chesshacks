# CORRECTED: Final Training Strategy (2 LC0 + 1 Transformer)

**CORRECTION:** The LeelaZeroTransformer in `chessformer.py` **DOES** support 112-channel LC0 data!

---

## ‚úÖ CORRECT 3-Model Strategy

### Model 1: LC0 128x6 (CNN Baseline)
- **Architecture:** CNN with 128 filters, 6 residual blocks
- **Parameters:** ~1.5M
- **Training time:** 1.5-2 hours
- **Expected ELO:** 1800-2000

### Model 2: LC0 128x10 (CNN Deeper)
- **Architecture:** CNN with 128 filters, 10 residual blocks
- **Parameters:** ~2.5M
- **Training time:** 2.5-3 hours
- **Expected ELO:** 1900-2100

### Model 3: LeelaZeroTransformer 256x6h8 (Hybrid)
- **Architecture:** Transformer with 256 filters, 6 layers, 8 heads
- **Parameters:** ~3-4M (transformer attention is more expensive)
- **Training time:** 2.5-3 hours (transformer attention slower than CNN)
- **Expected ELO:** 1850-2100 (transformers can excel at pattern recognition)

---

## üìä Why Transformer IS Compatible

### Proof from `src/models/chessformer.py`:

**Line 162:**
```python
self.input_block = ConvBlock(
    input_channels=112,  # ‚Üê ACCEPTS 112 CHANNELS!
    filter_size=3,
    output_channels=num_filters
)
```

**Line 186:**
```python
x = planes.reshape(B, 112, 8, 8)  # ‚Üê 112 CHANNEL INPUT!
```

**Architecture:**
1. **Input:** 112-channel ConvBlock (same as LC0)
2. **Backbone:** Transformer blocks (instead of residual blocks)
3. **Heads:** LC0 policy/value/moves_left heads

**Key insight:** It's a **hybrid** model combining:
- ‚úÖ LC0's proven input/output format
- ‚úÖ Transformer attention for pattern recognition
- ‚úÖ Full compatibility with LC0 data pipeline

---

## üöÄ Launch Commands (CORRECTED)

### One Command to Launch All 3:
```bash
bash training/scripts/launch_all_training.sh
```

### Or Individual Launches:

**Model 1: LC0 128x6**
```bash
modal run training/scripts/train_modal_lc0_v2.py \
  --num-epochs 10 \
  --batch-size 256 \
  --num-filters 128 \
  --num-residual-blocks 6 \
  --hf-repo steveandcow/chesshacks-lc0
```

**Model 2: LC0 128x10**
```bash
modal run training/scripts/train_modal_lc0_v2.py \
  --num-epochs 10 \
  --batch-size 256 \
  --num-filters 128 \
  --num-residual-blocks 10 \
  --hf-repo steveandcow/chesshacks-lc0
```

**Model 3: Transformer 256x6h8**
```bash
modal run training/scripts/train_modal_transformer_lc0.py \
  --num-epochs 10 \
  --batch-size 256 \
  --num-filters 256 \
  --num-blocks 6 \
  --heads 8 \
  --hf-repo steveandcow/chesshacks-lc0
```

---

## ‚è±Ô∏è Updated Time Estimates

### Model 3 Transformer Time Analysis

**Why slower than CNN:**
- Attention mechanism: O(n¬≤) where n=64 squares
- More expensive than convolution operations
- Larger model (~3-4M params vs ~2.5M for 128x10)

**Calculation:**
- Time per batch: ~30-35ms (vs ~25ms for 128x6)
- Per epoch: ~15-18 minutes
- Total (10 epochs): **2.5-3 hours**

**Why still worth it:**
- Transformers excel at long-range patterns
- Better at positional understanding
- Competitive with or better than CNNs in recent chess research
- Diversity in architecture types

---

## üìà Expected Performance Comparison

| Model | Type | Params | Time | Expected ELO | Strengths |
|-------|------|--------|------|--------------|-----------|
| 128x6 | CNN | 1.5M | 1.5-2h | 1800-2000 | Fast, reliable |
| 128x10 | CNN | 2.5M | 2.5-3h | 1900-2100 | Deep, strong |
| 256x6h8 | Transformer | 3-4M | 2.5-3h | 1850-2100 | Pattern recognition |

**Best strategy:** All 3 models complement each other
- CNN baseline (fast)
- CNN deep (strongest expected)
- Transformer (different approach, might surprise)

---

## üî¨ Research Support for Transformers in Chess

**Recent papers:**
- "Mastering Chess with a Transformer Model" (2024): CF-240M achieves competitive play
- Transformers excel at:
  - Long-range positional patterns
  - King safety evaluation
  - Piece coordination
  - Strategic planning

**Our hybrid approach:**
- Combines LC0's proven I/O with transformer attention
- Best of both worlds: CNN efficiency + transformer expressiveness

---

## ‚úÖ Files Created

1. **`train_modal_transformer_lc0.py`** - Transformer training script
   - Uses LeelaZeroTransformer from chessformer.py
   - Compatible with 112-channel LC0 data
   - All v2 fixes applied (shuffling, LR, dropout, patience, validation)

2. **`launch_all_training.sh`** - Updated to launch correct 3 models
   - Model 1: LC0 128x6
   - Model 2: LC0 128x10
   - Model 3: Transformer 256x6h8

---

## üìù Corrected Timeline

```
Hour 0:00 ‚Üí Launch all 3 models
Hour 0:05 ‚Üí All models start training
Hour 1:30 ‚Üí Model 1 (LC0 128x6) completes ‚úÖ
Hour 2:45 ‚Üí Model 2 (LC0 128x10) completes ‚úÖ
Hour 2:45 ‚Üí Model 3 (Transformer) completes ‚úÖ
Hour 3:00 ‚Üí ALL TRAINING COMPLETE
Hour 3-5  ‚Üí Deploy, test, iterate
```

**Total: ~3 hours** (wall-clock time for all 3 in parallel)

---

## üéØ Why This Strategy is Better

### Original (Incorrect) Strategy:
- ‚ùå 3 LC0 CNN models (128x6, 128x10, 192x6)
- Limited architectural diversity
- All same approach, just different sizes

### Corrected Strategy:
- ‚úÖ 2 LC0 CNN models (128x6, 128x10)
- ‚úÖ 1 Transformer model (256x6h8)
- **Better diversity:** CNN + Transformer approaches
- **Better coverage:** Different strengths for different positions
- **Research-backed:** Transformers proven effective in recent chess AI

---

## üí° Key Advantages of Including Transformer

### Complementary Strengths:

**CNNs excel at:**
- Local patterns (pawn structures)
- Tactical calculations
- Material evaluation
- Fast inference

**Transformers excel at:**
- Global patterns (king safety across board)
- Piece coordination
- Strategic plans
- Long-range threats

**Combined:** Deploy best of each to different slots!

---

## üöÄ READY TO LAUNCH

**Corrected understanding:**
- ‚úÖ All 3 models use same 112-channel LC0 data
- ‚úÖ No preprocessing needed
- ‚úÖ All use same fixed v2 pipeline
- ‚úÖ Architectural diversity maximized

**Launch now:**
```bash
bash training/scripts/launch_all_training.sh
```

---

## üìö Apology and Correction

**My mistake:** I incorrectly assumed the transformer used 16-channel data without checking the actual implementation.

**The truth:** `LeelaZeroTransformer` is specifically designed as a hybrid:
- LC0's 112-channel input
- Transformer attention backbone
- LC0's output heads

**Lesson:** Always verify code before making assumptions! Thank you for catching this error.

---

## ‚úÖ Final Checklist

- [ ] Modal authenticated
- [ ] Data at `/data/lc0_processed` (112-channel)
- [ ] HuggingFace secret configured
- [ ] All 3 training scripts ready:
  - ‚úÖ `train_modal_lc0_v2.py` (CNN)
  - ‚úÖ `train_modal_transformer_lc0.py` (Transformer)
- [ ] Launch script updated: `launch_all_training.sh`

**LAUNCH:**
```bash
bash training/scripts/launch_all_training.sh
```

**Expected:** All 3 models complete in ~3 hours, giving 2-hour buffer for deployment! üöÄ
