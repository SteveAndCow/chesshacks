# Full Transformer Configuration
# Larger model for better performance (if you have time/compute)

model:
  type: "transformer"
  params:
    d_model: 256
    nhead: 8
    num_layers: 4
    dim_feedforward: 1024
    dropout: 0.1

training:
  batch_size: 64  # Smaller batch for larger model
  epochs: 20
  lr: 0.00005
  weight_decay: 0.0001

  # Loss weights
  policy_weight: 1.0
  value_weight: 1.0
  result_weight: 0.5  # Auxiliary task for win/draw/loss classification

  # Learning rate schedule
  lr_scheduler: "cosine"
  warmup_epochs: 3

data:
  data_dir: "/data/processed"  # Modal volume path
  num_workers: 0  # Modal handles parallelism
  train_split: 0.9
  max_samples: null  # Use all available samples for best performance

# HuggingFace upload settings
huggingface:
  repo_id: "steveandcow/chesshacks-bot"
  model_name: "transformer_full"
