# Full Transformer Configuration
# Larger model for better performance (if you have time/compute)

model:
  type: "transformer"
  params:
    d_model: 256
    nhead: 8
    num_layers: 4
    dim_feedforward: 1024
    dropout: 0.1

training:
  batch_size: 64  # Smaller batch for larger model
  epochs: 20
  lr: 0.00005
  weight_decay: 0.0001

  # Loss weights
  policy_weight: 1.0
  value_weight: 1.0

  # Learning rate schedule
  lr_scheduler: "cosine"
  warmup_epochs: 3

data:
  data_dir: "training/data/processed"
  num_workers: 4
  train_split: 0.9

# HuggingFace upload settings
huggingface:
  repo_id: "your-username/chesshacks-bot"  # Change this!
  model_name: "transformer_full"
