# Tiny Transformer Configuration
# ChessFormer-inspired architecture scaled down for 36-hour hackathon

model:
  type: "transformer_lite"  # or "transformer" for full version
  params:
    d_model: 128
    nhead: 4
    num_layers: 2
    dim_feedforward: 512
    dropout: 0.1

training:
  batch_size: 128  # Smaller batch for transformer
  epochs: 15
  lr: 0.0001  # Lower LR for transformer
  weight_decay: 0.0001

  # Loss weights
  policy_weight: 1.0
  value_weight: 1.0
  result_weight: 0.5  # Auxiliary task for win/draw/loss classification

  # Learning rate schedule
  lr_scheduler: "cosine"  # Cosine annealing works well for transformers
  warmup_epochs: 2

data:
  data_dir: "/data/processed"  # Modal volume path
  num_workers: 0  # Modal handles parallelism
  train_split: 0.9
  max_samples: 500000  # Use 500k samples for better training

# HuggingFace upload settings
huggingface:
  repo_id: "steveandcow/chesshacks-bot"
  model_name: "transformer_tiny"
